{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPs5+rIiraWR3DiQOJIgejT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arieljumba/Colab-Projects/blob/main/G_ML_CRASH_COURSE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD6p1EnPLLsz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#G MACHINE LEARNING CRASH COURSE\n",
        "\n",
        "##Linear Regression\n",
        "\n",
        "x = label\n",
        "y = feature\n",
        "the model describes the relationship between feature and label\n",
        "\n",
        "True, the line doesn't pass through every dot, but the line does clearly show the relationship between chirps and temperature. Using the equation for a line, you could write down this relationship as follows:\n",
        "\n",
        "where:  $y = mx + b$\n",
        "\n",
        " - $y$ is the temperature in Celsius—the value we're trying to predict.\n",
        " - $m$ is the slope of the line.\n",
        " - $x$ is the number of chirps per minute—the value of our input feature.\n",
        " - $b$ is the y-intercept.\n",
        "<br>\n",
        "<br>\n",
        "By convention in machine learning, you'll write the equation for a model slightly differently: \n",
        "\n",
        "where: $y=b+w_{1}+x_{1}$\n",
        "\n",
        " - $y$ is the predicted label (a desired output).\n",
        " - $b$ is the bias (the y-intercept), sometimes referred to as $w_{0}$\n",
        " - $w_{1}$ is the weight of feature 1. Weight is the same concept as the \"slope\" in the traditional equation of a line.\n",
        " - $x_{1}$ is a feature (a known input).\n",
        "\n",
        "### Training & Loss\n",
        "\n",
        "Training a model simply means learning (determining) good values for all the weights and the bias from labeled examples. <br>\n",
        "Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples\n",
        "\n",
        "Mean square error (MSE)\n",
        "<br>\n",
        "\n",
        "$MSE = \\frac{1}{N} ∑ \\limits_{(x,y)}(y-prediction(x))^{2} $\n",
        "\n",
        "<br>\n",
        "Although MSE is commonly-used in machine learning, it is neither the only practical loss function nor the best loss function for all circumstances."
      ],
      "metadata": {
        "id": "o6J-e86mLccL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wleHIBOePC40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Hat: \\hat\n",
        "- Subscript: _{}\n",
        "- Sum: \\sum\n",
        "- Limits of sum: \\limits _{} ^{}\n",
        "- Beta: \\beta"
      ],
      "metadata": {
        "id": "moeL_tnNPDW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7v3mmYWTLvTs"
      }
    }
  ]
}